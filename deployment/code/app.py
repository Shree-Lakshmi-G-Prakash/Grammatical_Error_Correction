# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xKsRf-9QjUNCt2LmNVSz4JjNuFbhXqwX
"""

from  flask import Flask, jsonify,render_template, request

#import matplotlib.pyplot as plt
#import matplotlib.ticker as ticker
#from sklearn.model_selection import train_test_split
#import nltk.translate.gleu_score as gleu

import unicodedata
import re
import numpy as np
import os
import io
import time
import pickle
import random
import pandas as pd
from tqdm import tqdm

import tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import  pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, LSTMCell
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TensorBoard

import tensorflow_addons as tfa


from encoder_decoder import Encoder_Decoder_Attention

app = Flask(__name__)

#load pre requisities 

MAX_SEQ_LEN = 10

train_encoder_inp = np.load('../data/sample_encoder_inp.npy')
train_decoder_inp = np.load('../data/sample_decoder_inp.npy')
train_decoder_out = np.load('../data/sample_decoder_out.npy')


encoder_embeddings = np.load('../data/encoder_embeddings.npy')
decoder_embeddings = np.load('../data/decoder_embeddings.npy')


with open ('../checkpoints/tokenizer_encoder_word_level.pickle', 'rb') as fp:
  tokenizer_encoder = pickle.load(fp)

with open ('../checkpoints/tokenizer_decoder_word_level.pickle', 'rb') as fp:
  tokenizer_decoder = pickle.load(fp)

#optimizer and loss function

optimizer = tensorflow.keras.optimizers.Adam()

#defining custom loss function which will not consider loss for padded zeroes
# code taken from attention assignment
loss_object = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
def loss_function(real, pred):
  mask = tensorflow.math.logical_not(tensorflow.math.equal(real, 0))
  loss_ = loss_object(real, pred)

  mask = tensorflow.cast(mask, dtype=loss_.dtype)
  loss_ *= mask

  return tensorflow.reduce_mean(loss_)

#training parameters
inp_vocab_size = encoder_embeddings.shape[0]
out_vocab_size = decoder_embeddings.shape[0] 

emb_dim = encoder_embeddings.shape[1]

en_inp_len = MAX_SEQ_LEN
de_inp_len = MAX_SEQ_LEN+1

lstm_size=64
batch_size=4

#load model

model = Encoder_Decoder_Attention(inp_vocab_size, emb_dim, en_inp_len, lstm_size,\
                                   out_vocab_size, emb_dim, de_inp_len, lstm_size,\
                                   batch_size,attention='bahdanau',\
                                   en_embeddings=encoder_embeddings, en_emb_trainable=True,
                                   de_embeddings=decoder_embeddings, de_emb_trainable=True)

model.compile(optimizer=optimizer, loss=loss_function)

model.train_on_batch([train_encoder_inp[:batch_size], train_decoder_inp[:batch_size]], train_decoder_out[:batch_size])
model.load_weights('../checkpoints/Attention/bahdanau_model.ckpt')

def decontracted(phrase):
    # specific
    phrase = re.sub(r"won\'t", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    return phrase

def clean(text):
    """
    takes string as input and
    removes characters inside (),{},[] and <>
    removes characters like -+@#^/|*(){}$~`
    we not not removing ,.!-:;"' as these characters are present in english language 
    """
    text = re.sub('<.*>', '', text)
    text = re.sub('\(.*\)', '', text)
    text = re.sub('\[.*\]', '', text)
    text = re.sub('{.*}', '', text)
    text = re.sub("[-+@#^/|*(){}$~<>=_%:;]","",text)
    text = text.replace("\\","")
    text = re.sub("\[","",text)
    text = re.sub("\]","",text)
    text = re.sub("[0-9]","",text)
    text = text.lower()
    text = text.strip()
    return text

def beam_evaluate_sentence(sentences, model, beam_width):

  inputs = tokenizer_encoder.texts_to_sequences(sentences)
  inputs = pad_sequences(inputs, maxlen=MAX_SEQ_LEN, padding='post', truncating='post', value=0)
  inputs = tensorflow.convert_to_tensor(inputs)

  inference_batch_size = inputs.shape[0]
  result=''

  #encoder
  encoder_start_state = [tensorflow.zeros(shape=(inference_batch_size, lstm_size)), tensorflow.zeros(shape=(inference_batch_size, lstm_size))]
  en_out, en_h, en_c = model.layers[0](inputs, encoder_start_state)

  dec_h, dec_c = en_h, en_c

  start_tokens = tensorflow.fill([inference_batch_size], tokenizer_decoder.word_index['<start>'])
  end_token = tokenizer_decoder.word_index['<end>']

  #tile encoder output
  en_out_tiled = tfa.seq2seq.tile_batch(en_out, multiplier=beam_width)
  model.layers[1].attention_mechanism.setup_memory(en_out_tiled)

  #tile encoder hidden state
  hidden_states = tfa.seq2seq.tile_batch([en_h, en_c], multiplier=beam_width)
  decoder_initial_state = model.layers[1].rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tensorflow.float32)
  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_states)

  decoder_instance = tfa.seq2seq.BeamSearchDecoder(model.layers[1].rnn_cell, beam_width=beam_width, output_layer=model.layers[1].fc)

  decoder_embedding_matrix = model.layers[1].embedding.variables[0]

  outputs, final_state, seq_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token,\
                                                       initial_state=decoder_initial_state)
  
  final_outputs = tensorflow.transpose(outputs.predicted_ids, perm=(0,2,1))
  beam_scores = tensorflow.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))

  return final_outputs.numpy(), beam_scores.numpy()

def grammatical_error_correction(sentence, model):
  result, beam_score = beam_evaluate_sentence(sentence, model, beam_width=3)
  output = tokenizer_decoder.sequences_to_texts(result[0])

  return output[0]

@app.route('/')
def hellword():
  print('Hello word')
  return jsonify({'message':'Hello World!!'})

@app.route('/index')
def index():
  return render_template('index.html')

@app.route('/predict', methods=['post'])
def predict():
  print("Inside predict function")
  sent = request.form['user_text']
  print("Input:  ", sent)
  sent = decontracted(sent)
  sent = clean(sent)
  corrected = grammatical_error_correction([sent], model)

  result = {'CORRECTED':corrected}
  print("output:  ",result)
  return jsonify(result)


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)